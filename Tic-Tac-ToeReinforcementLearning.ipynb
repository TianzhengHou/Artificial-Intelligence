{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "class ESAgent:\n",
    "    def __init__(self,weights = None, bias = None):\n",
    "        \"\"\"\n",
    "            initialize agent with weight and bias\n",
    "\n",
    "            The agent should carry a MLP network where it has 16 features input and 9 output.\n",
    "\n",
    "            Hence, MLP has one input layer (16 weights), and input layer fully connects to one output layer (9 weights and 9 bias).\n",
    "\n",
    "            In order to let ESAgent able to inherite network from its parent. The arguments should have default value for inilizate new population.\n",
    "        Args:\n",
    "            weights : weights of network or None\n",
    "            bias : bias of network or None\n",
    "        \"\"\"\n",
    "        if weights is not None:\n",
    "            self.weights = weights\n",
    "        else:\n",
    "            self.weights = np.random.uniform(-1, 1, (16, 9))\n",
    "\n",
    "        if bias is not None:\n",
    "            self.bias = bias\n",
    "        else:\n",
    "            self.bias = np.random.uniform(-1, 1, 9)\n",
    "\n",
    "\n",
    "    def make_a_move(self,board):\n",
    "        \"\"\"make a move respect to current board state. You would like to use feature_construct function to convert board state to input features\n",
    "\n",
    "        Args:\n",
    "            board: tic tak toe game state\n",
    "\n",
    "        Returns:\n",
    "            move (0-8): position of board\n",
    "\n",
    "        \"\"\"\n",
    "        features = feature_construct(board)\n",
    "        net = np.dot(features, self.weights) + self.bias\n",
    "        output = 1 / (1 + np.exp(-net))\n",
    "        output[board.flatten() != 0] = -np.inf\n",
    "        move = np.argmax(output)\n",
    "        return move\n",
    "    \n",
    "# dont change any code after this\n",
    "class RuleBaseAgent:\n",
    "    def __init__(self,id,rival_id,p_rnd=0.1):\n",
    "        self.p_rnd = p_rnd\n",
    "        self.move = -1\n",
    "        self.id = id\n",
    "        self.rival_id = rival_id\n",
    "    \n",
    "    def make_a_move(self,board):\n",
    "        self.find_avaliable_position(board)\n",
    "        if np.random.random() < self.p_rnd:\n",
    "            self.random_move()\n",
    "        elif self.make_win_move(board):\n",
    "            pass\n",
    "        elif self.make_block_move(board):\n",
    "            pass\n",
    "        elif self.make_two_open_move(board):\n",
    "            pass\n",
    "        else: \n",
    "            self.random_move()\n",
    "        self.avaliable_moves = None\n",
    "        return self.move\n",
    "        \n",
    "    def find_avaliable_position(self,board):\n",
    "        self.avaliable_moves = [i for i in range(9) if board[i//3][i%3] == 0]\n",
    "\n",
    "    def random_move(self):\n",
    "        move = np.random.choice(self.avaliable_moves)\n",
    "        # move = self.avaliable_moves[0]\n",
    "        self.move = (move//3,move%3)\n",
    "\n",
    "    def make_win_move(self,board):\n",
    "        for i,row in enumerate(board):\n",
    "            if row.sum() == 2 * self.id:\n",
    "                for j,value in enumerate(row):\n",
    "                    if value == 0:\n",
    "                        self.move= (i,j)\n",
    "                        return True\n",
    "                    \n",
    "        \n",
    "        for j,col in enumerate(board.T):\n",
    "            if col.sum() == 2 * self.id:\n",
    "                for i,value in enumerate(col):\n",
    "                    if value == 0:\n",
    "                        self.move= (i,j) \n",
    "                        return True\n",
    "                    \n",
    "        if board.trace() == 2 * self.id:\n",
    "            for i in range(3):\n",
    "                if board[i][i] == 0:\n",
    "                    self.move = (i,i)\n",
    "                    return True\n",
    "        \n",
    "        if np.fliplr(board).trace() == 2 * self.id:\n",
    "            for i in range(3):\n",
    "                if board[i][2-i] == 0:\n",
    "                    self.move = (i,2-i)\n",
    "                    return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def make_block_move(self,board):\n",
    "        for i,row in enumerate(board):\n",
    "            if row.sum() == 2 * self.rival_id:\n",
    "                for j,value in enumerate(row):\n",
    "                    if value == 0:\n",
    "                        self.move= (i,j)\n",
    "                        return True\n",
    "                    \n",
    "        for j,col in enumerate(board.T):\n",
    "            if col.sum() == 2 * self.rival_id:\n",
    "                for i,value in enumerate(col):\n",
    "                    if value == 0:\n",
    "                        self.move= (i,j)\n",
    "                        return True\n",
    "    \n",
    "        if board.trace() == 2 * self.rival_id:\n",
    "            for i in range(3):\n",
    "                if board[i][i] == 0:\n",
    "                    self.move = (i,i)\n",
    "                    return True\n",
    "        \n",
    "        if np.fliplr(board).trace() == 2 * self.rival_id:\n",
    "            for i in range(3):\n",
    "                if board[i][2-i] == 0:\n",
    "                    self.move = (i,2-i)\n",
    "                    return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def make_two_open_move(self,board):\n",
    "        p = 0.5\n",
    "        if board.trace() == self.id:\n",
    "            for i in range(3):\n",
    "                if board[i][i] == 0:\n",
    "                    if p < np.random.random():\n",
    "                        self.move = (i,i)\n",
    "                        return True\n",
    "                    else:\n",
    "                        p = 1\n",
    "        \n",
    "        p = 0.5\n",
    "        if np.fliplr(board).trace() == self.id:\n",
    "            for i in range(3):\n",
    "                if board[i][2-i] == 0:\n",
    "                    if p < np.random.random():\n",
    "                        self.move = (i,2-i)\n",
    "                        return True\n",
    "                    else:\n",
    "                        p = 1\n",
    "        \n",
    "        p = 0.5\n",
    "        for i,row in enumerate(board):\n",
    "            if row.sum() == self.id:\n",
    "                for j,value in enumerate(row):\n",
    "                    if value == 0:\n",
    "                        if p < np.random.random():\n",
    "                            self.move= (i,j)\n",
    "                            return True\n",
    "                        else:\n",
    "                            p = 1\n",
    "                    \n",
    "        \n",
    "        p = 0.5\n",
    "        for j,col in enumerate(board.T):\n",
    "            if col.sum() == self.id:\n",
    "                for i,value in enumerate(col):\n",
    "                    if value == 0:\n",
    "                        if p < np.random.random():\n",
    "                            self.move= (i,j)\n",
    "                            return True\n",
    "                        else:\n",
    "                            p = 1\n",
    "\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "class TestAgent:\n",
    "    def test_ESAgent_network(self):\n",
    "        agent = ESAgent()\n",
    "\n",
    "        assert agent.weights.shape == (16,9)\n",
    "        assert agent.bias.shape == (9,)\n",
    "\n",
    "    def test_ESAgent_network(self):\n",
    "        agent = ESAgent()\n",
    "        \n",
    "        board = np.array([[0,1,1],\n",
    "                          [1,-1,-1],\n",
    "                          [-1,-1,1]\n",
    "                          ])\n",
    "        move = agent.make_a_move(board)\n",
    "        assert move == 0\n",
    "\n",
    "        board = np.array([[-1,1,1],\n",
    "                          [1,0,-1],\n",
    "                          [-1,-1,1]\n",
    "                          ])\n",
    "        move = agent.make_a_move(board)\n",
    "        assert move == 4\n",
    "        \n",
    "        board = np.array([[-1,1,1],\n",
    "                          [1,-1,-1],\n",
    "                          [0,-1,1]\n",
    "                          ])\n",
    "        move = agent.make_a_move(board)\n",
    "        assert move == 6\n",
    "\n",
    "class TestModel:\n",
    "    def test_model_play(self):\n",
    "        em = Evolutionary_Model(max_pop=1000,parent_percent=0.3)\n",
    "        best_agent = em.play_tic_tak_toe(max_epoch=100)\n",
    "\n",
    "        num_trials = 1000\n",
    "        wins = 0\n",
    "        loss = 0\n",
    "        draw = 0\n",
    "        currTrial = 0\n",
    "\n",
    "        for _ in range(num_trials):\n",
    "            currTrial += 1\n",
    "            env = TicTakToe(rng=0)\n",
    "            while not env.terminate:\n",
    "                action = best_agent.make_a_move(env.board)\n",
    "                env.step(action)\n",
    "            if env.winner == Player.PLAYER1:\n",
    "                wins += 1\n",
    "            elif env.winner == Player.PLAYER2:\n",
    "                loss += 1\n",
    "            else:\n",
    "                draw += 1\n",
    "            env.reset()\n",
    "\n",
    "        print(\"wins:\",wins/num_trials,\"loss\",loss/num_trials,\"draw\",draw/num_trials)\n",
    "\n",
    "        assert True\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "    \n",
    "class TicTakToe():\n",
    "    \"\"\"\n",
    "        tic take tow environment. you dont need to change any code here\n",
    "\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self,rng):\n",
    "        \"\"\"initilize opponent rule base agent\n",
    "\n",
    "        Args:\n",
    "            rng (_type_): it the number of chance opponents make random move\n",
    "        \"\"\"\n",
    "        self.player2 = RuleBaseAgent(id=Player.PLAYER2,rival_id=Player.PLAYER1,p_rnd=rng)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "            reset game state\n",
    "        \"\"\"\n",
    "        self.board = np.zeros((3,3)).astype('int')\n",
    "        self.prev_board = copy.deepcopy(self.board)\n",
    "        self.curr_player = Player.PLAYER1\n",
    "        self.round = 1\n",
    "        self.winner = 0\n",
    "        self.terminate = False\n",
    "\n",
    "    def step(self,action):\n",
    "        \"\"\"environment take a action from player 1 (in this case will be your agent), and it will take this move and call opponent agent also move. \n",
    "\n",
    "        Args:\n",
    "            action      : the position of board\n",
    "\n",
    "        Returns:\n",
    "            board       : the board state\n",
    "            prev_board  : the prev board state (no useful in assignment 2)\n",
    "            terminate   : if game terminated\n",
    "            self.winner : winner of game if terminated, Player 1 -> player 1 win, Player 2 -> player 2 win, 0 -> draw\n",
    "        \"\"\"\n",
    "        if self.terminate:\n",
    "            return self.board,self.prev_board,self.terminate,self.winner\n",
    "\n",
    "        if not isinstance(action,list) and not isinstance(action, np.ndarray) and not isinstance(action, tuple):\n",
    "            x,y = (action//3,action%3)\n",
    "        elif len(action) == 2:\n",
    "            x,y = action\n",
    "        else:\n",
    "            print(\"invalid input\")\n",
    "            assert RuntimeError\n",
    "        \n",
    "        self.move(x,y)\n",
    "        self.update_round()\n",
    "\n",
    "        if not self.terminate:\n",
    "            self.prev_board = copy.deepcopy(self.board)\n",
    "            action = self.player2.make_a_move(self.board)\n",
    "            x,y = action\n",
    "            self.move(x,y)\n",
    "            self.update_round()\n",
    "\n",
    "        return self.board,self.prev_board,self.terminate,self.winner\n",
    "    \n",
    "    def switch_player(self):\n",
    "        if self.curr_player == Player.PLAYER1:\n",
    "            self.curr_player = Player.PLAYER2\n",
    "        else:\n",
    "            self.curr_player = Player.PLAYER1\n",
    "\n",
    "    def move(self,x,y):\n",
    "        if x < 0 or x > 2 or y < 0 or y > 2:\n",
    "            print(\"out of boundary.\")\n",
    "            return False\n",
    "        elif self.board[x][y] != 0:\n",
    "            print(\"occupied.\")\n",
    "            return False\n",
    "        else:\n",
    "            self.board[x][y] = self.curr_player\n",
    "            self.last_move = (x,y)\n",
    "\n",
    "            return True\n",
    "\n",
    "    def is_win(self):\n",
    "        x,y = self.last_move\n",
    "        rows_check = self.board[x].sum() == 3 * self.curr_player\n",
    "        cols_check = self.board[:,y].sum() == 3 * self.curr_player\n",
    "\n",
    "        left_diagonal_check = False\n",
    "        if x == y:\n",
    "            left_diagonal_check = np.trace(self.board) == 3 * self.curr_player\n",
    "\n",
    "        right_diagonal_check = False\n",
    "        if x + y == 2:\n",
    "            right_diagonal_check = np.trace(np.fliplr(self.board)) == 3 * self.curr_player\n",
    "\n",
    "        return rows_check or cols_check or left_diagonal_check or left_diagonal_check or right_diagonal_check\n",
    "    \n",
    "    def update_round(self):\n",
    "        if self.is_win():\n",
    "            self.winner = self.curr_player\n",
    "            self.terminate = True\n",
    "        \n",
    "        self.round += 1\n",
    "        if self.round > 9:\n",
    "            self.terminate = True\n",
    "        else:\n",
    "            self.switch_player()\n",
    "\n",
    "    def show(self):\n",
    "        \"\"\"\n",
    "            print the board state\n",
    "        \"\"\"\n",
    "        print(self.board)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "class Evolutionary_Model:\n",
    "    def __init__(self,max_pop = 1000, parent_percent = 0.2):\n",
    "        \"\"\"initilize evolutionary model and first population\n",
    "\n",
    "        Args:\n",
    "            max_pop (int, optional): maximum population at each epoch. Defaults to 1000.\n",
    "            parent_percent (float, optional): the percentage of parent after selection. Defaults to 0.2.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.max_pop = max_pop\n",
    "        self.parent_percent = parent_percent\n",
    "\n",
    "        self.best_agent = None\n",
    "        self.best_reward = -1000\n",
    "        self.population = self.initial_population()\n",
    "\n",
    "    def play_tic_tak_toe(self, max_epoch = 100):\n",
    "        \"\"\"play tic tak toe evolutionary and return the best agent you have\n",
    "\n",
    "        you should follow the persudocode in slides but you allow to some changes as long as it return the best agent you have in the model.\n",
    "\n",
    "        1. intilize population\n",
    "        2. do\n",
    "        3.  fitness         <- evaluation\n",
    "        3.  parent          <- selection\n",
    "        4.  new_population  <- evolution\n",
    "        5. return best_agent\n",
    "\n",
    "        Returns:\n",
    "            best_agent (ESagent): _description_\n",
    "        \"\"\"\n",
    "\n",
    "        for epoch in range(max_epoch):\n",
    "            rewords = []\n",
    "            for agent in self.population:\n",
    "                reword = self.evaluation(TicTakToe(rng=0),agent,num_trials=100)\n",
    "                rewords.append(reword)\n",
    "                if reword > self.best_reward:\n",
    "                    self.best_reward = reword\n",
    "                    self.best_agent = agent\n",
    "            parent = self.selection(rewords,self.population)\n",
    "            self.population = self.evolution(parent)\n",
    "        return self.best_agent\n",
    "    \n",
    "    def initial_population(self):\n",
    "        \"\"\"initilize first population\n",
    "\n",
    "        Returns:\n",
    "            population (ESAgent[]): Array of Agents\n",
    "        \"\"\"\n",
    "        return [ESAgent() for _ in range(self.max_pop)]\n",
    "    \n",
    "    def evaluation(self,env,agent,num_trials):\n",
    "        \"\"\"\n",
    "            evaluate the reward for each agent. feel free to have your own reward function.\n",
    "        \"\"\"\n",
    "        total_reword = 0\n",
    "        for _ in range(num_trials):\n",
    "            env.reset()\n",
    "            while not env.terminate:\n",
    "                action = agent.make_a_move(env.board)\n",
    "                env.step(action)\n",
    "            if env.winner == Player.PLAYER1:\n",
    "                total_reword += 1\n",
    "            elif env.winner == Player.PLAYER2:\n",
    "                total_reword -= 2\n",
    "            else:\n",
    "                total_reword += 0\n",
    "        return total_reword/num_trials\n",
    "\n",
    "    def selection(self,rewards,population):\n",
    "        \"\"\"\n",
    "            select the best fit in the population. feel free to have your own selection.\n",
    "            Make sure you select parent according to parent_percent\n",
    "        \"\"\"\n",
    "        rewards = np.array(rewards)\n",
    "        population = np.array(population)\n",
    "        sorted_indices = np.argsort(rewards)\n",
    "        rewards = rewards[sorted_indices]\n",
    "        sorted_population = population[sorted_indices]\n",
    "        parent_size = int(self.parent_percent * len(population))\n",
    "        return sorted_population[:parent_size]\n",
    "\n",
    "\n",
    "    def evolution(self,parents):\n",
    "        \"\"\"\n",
    "            evolute new population from parents. \n",
    "\n",
    "            be careful about how to reinforce children. You don't want your children perform same as parents and even worser than parents.\n",
    "\n",
    "            feel free to have your own evolution. In MLP case, you would like to add some noises to weights and bias.\n",
    "        \"\"\"\n",
    "        children = []\n",
    "        for _ in range(len(parents)):\n",
    "            parent = np.random.choice(parents)\n",
    "            child_weights = parent.weights + np.random.normal(0, 0.1, size=parent.weights.shape)\n",
    "            child_bias = parent.bias + np.random.normal(0, 0.1, size=parent.bias.shape)\n",
    "            child = ESAgent(weights=child_weights, bias=child_bias)\n",
    "            children.append(child)\n",
    "        return np.concatenate((parents, children))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import IntEnum\n",
    "\n",
    "class Player(IntEnum):\n",
    "    PLAYER1 = 1\n",
    "    PLAYER2 = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def feature_construct(board):\n",
    "    \"\"\"convert board state to 16 features\n",
    "\n",
    "    Args:\n",
    "        board (_type_): board state\n",
    "\n",
    "    Returns:\n",
    "        features: 16 input features\n",
    "    \"\"\"\n",
    "    row = board.sum(axis=0)\n",
    "    col = board.sum(axis=1)\n",
    "\n",
    "    a_row = row + 1\n",
    "    b_row = row - 1\n",
    "    a_col = col + 1\n",
    "    b_col = col - 1\n",
    "\n",
    "    dig = board.trace()\n",
    "    flip_dig = np.fliplr(board).trace()\n",
    "\n",
    "    a_dig = dig + 1\n",
    "    a_flip_dig = flip_dig + 1\n",
    "    b_dig = dig - 1\n",
    "    b_flip_dig = flip_dig - 1\n",
    "\n",
    "    features = np.concatenate([a_row,b_row,a_col,b_col,[a_dig,b_dig,a_flip_dig,b_flip_dig]])\n",
    "    features = 1/(1 + np.exp(-features))\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wins: 0.611 loss 0.133 draw 0.256\n"
     ]
    }
   ],
   "source": [
    "TestAgent.test_ESAgent_network(None)\n",
    "TestModel.test_model_play(None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
